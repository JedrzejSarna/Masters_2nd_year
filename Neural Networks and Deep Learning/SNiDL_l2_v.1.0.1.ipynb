{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sieci neuronowe i Deep Learning\n",
    "# Temat 2: Adaline i metoda gradientu prostego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.1\n",
    "\n",
    "Rozważyć cztery scenariusze uczenia modelu Adaline na rozważanym zbiorze danych Iris (2 cechy i 2 klasy):\n",
    "1. Adaline z wykorzystaniem wsadowej wersji metody gradientu prostego, bez standaryzacji cech,\n",
    "2. Adaline z wykorzystaniem wsadowej wersji metody gradientu prostego, ze standaryzacją cech,\n",
    "3. Adaline z wykorzystaniem wersji SGD metody gradientu prostego (aproksymacja funkcji celu na podstawie jednego przykładu), bez standaryzacji cech,\n",
    "4. Adaline z wykorzystaniem wersji SGD metody gradientu prostego (aproksymacja funkcji celu na podstawie jednego przykładu), ze standaryzacją cech.\n",
    "\n",
    "Dla każdego scenariusza znaleźć optymalny parametr uczenia, a następnie porównać te scenariusze pod kątem szybkości uczenia się sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.2\n",
    "\n",
    "Wybrać najlepsze z czterech rozważanych w ramach poprzedniego zadania podejść i zbudować model dla pełnego zbioru danych Iris (4 cechy, 3 klasy).\n",
    "\n",
    "W celu poradzenia sobie z trzema klasami wykorzystać podejście OvA (one-versus-all)\n",
    "(czasami nazywana także one-versus-rest, OvR).\n",
    "\n",
    "Trenujemy po jednym klasyfikatorze dla każdej klasy -\n",
    "wybrana klasa jest traktowana jako klasa pozytywna, a pozostałe klasy uznawane są za klasy negatywne.\n",
    "\n",
    "Klasyfikacja polega na użyciu wszystkich klasyfikatorów ($k$ klasyfikatorów dla $k$ klas)\n",
    "i wybraniu klasy, która według tych klasyfikatorów jest ,,najbardziej prawdopodobna\" (ma najwyższy wynik).\n",
    "\n",
    "Obliczyć błąd finalnego modelu na danych uczących (czyli w tym przypadku na całym zbiorze danych)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.3*\n",
    "\n",
    "Zmodyfikować definicję klasy Adaline tak, aby do uczenia modelu wykorzystywała metodę gradientu prostego z minigrupami (rozmiar minigrup jako parametr metody):\n",
    "w każdym przebiegu gradient obliczany jest na podstawie niewielkich grup losowo dobieranych danych.\n",
    "\n",
    "Porównać działanie metody dla różnych rozmiarów minigrup (zaczynając od rozmiaru jeden, czyli od standardowej wersji SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.4*\n",
    "\n",
    "Zbadać zmianę jakości modelu (w sensie dopasowania do danych uczących) wraz ze wzrostem rozmiaru danych. Wykorzystać metodę `partial_fit` do douczania modelu przykład po przykładzie.\n",
    "\n",
    "Wyniki przedstawić w formie wykresu (jakość od rozmiaru danych)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.5*\n",
    "\n",
    "Powtórzyć poprzednie zadanie, tylko tym razem jakość modelu badać na odłożony zbiorze testowym rozmiaru $30\\%$ całego zbioru danych."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
